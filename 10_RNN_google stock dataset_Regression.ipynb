{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xpAuMVCwfWs8"
   },
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxJfRe4bfYVA"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ir9zwETrfbrp"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZT1f24vHffuf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nQ47JAxrgmaL"
   },
   "source": [
    "### Importing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xiv3pJOgqY3"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Google_Stock_Price_Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/3/2012</td>\n",
       "      <td>325.25</td>\n",
       "      <td>332.83</td>\n",
       "      <td>324.97</td>\n",
       "      <td>663.59</td>\n",
       "      <td>7,380,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/4/2012</td>\n",
       "      <td>331.27</td>\n",
       "      <td>333.87</td>\n",
       "      <td>329.08</td>\n",
       "      <td>666.45</td>\n",
       "      <td>5,749,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/5/2012</td>\n",
       "      <td>329.83</td>\n",
       "      <td>330.75</td>\n",
       "      <td>326.89</td>\n",
       "      <td>657.21</td>\n",
       "      <td>6,590,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/6/2012</td>\n",
       "      <td>328.34</td>\n",
       "      <td>328.77</td>\n",
       "      <td>323.68</td>\n",
       "      <td>648.24</td>\n",
       "      <td>5,405,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/9/2012</td>\n",
       "      <td>322.04</td>\n",
       "      <td>322.29</td>\n",
       "      <td>309.46</td>\n",
       "      <td>620.76</td>\n",
       "      <td>11,688,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>12/23/2016</td>\n",
       "      <td>790.90</td>\n",
       "      <td>792.74</td>\n",
       "      <td>787.28</td>\n",
       "      <td>789.91</td>\n",
       "      <td>623,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>12/27/2016</td>\n",
       "      <td>790.68</td>\n",
       "      <td>797.86</td>\n",
       "      <td>787.66</td>\n",
       "      <td>791.55</td>\n",
       "      <td>789,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>12/28/2016</td>\n",
       "      <td>793.70</td>\n",
       "      <td>794.23</td>\n",
       "      <td>783.20</td>\n",
       "      <td>785.05</td>\n",
       "      <td>1,153,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>12/29/2016</td>\n",
       "      <td>783.33</td>\n",
       "      <td>785.93</td>\n",
       "      <td>778.92</td>\n",
       "      <td>782.79</td>\n",
       "      <td>744,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>12/30/2016</td>\n",
       "      <td>782.75</td>\n",
       "      <td>782.78</td>\n",
       "      <td>770.41</td>\n",
       "      <td>771.82</td>\n",
       "      <td>1,770,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date    Open    High     Low   Close      Volume\n",
       "0       1/3/2012  325.25  332.83  324.97  663.59   7,380,500\n",
       "1       1/4/2012  331.27  333.87  329.08  666.45   5,749,400\n",
       "2       1/5/2012  329.83  330.75  326.89  657.21   6,590,300\n",
       "3       1/6/2012  328.34  328.77  323.68  648.24   5,405,900\n",
       "4       1/9/2012  322.04  322.29  309.46  620.76  11,688,800\n",
       "...          ...     ...     ...     ...     ...         ...\n",
       "1253  12/23/2016  790.90  792.74  787.28  789.91     623,400\n",
       "1254  12/27/2016  790.68  797.86  787.66  791.55     789,100\n",
       "1255  12/28/2016  793.70  794.23  783.20  785.05   1,153,800\n",
       "1256  12/29/2016  783.33  785.93  778.92  782.79     744,300\n",
       "1257  12/30/2016  782.75  782.78  770.41  771.82   1,770,000\n",
       "\n",
       "[1258 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = df_train.iloc[:, 1:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[325.25],\n",
       "       [331.27],\n",
       "       [329.83],\n",
       "       ...,\n",
       "       [793.7 ],\n",
       "       [783.33],\n",
       "       [782.75]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HT8_2UJegtG5"
   },
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OTrF2kR7gx9x"
   },
   "outputs": [],
   "source": [
    "#for RNN it is recommended to do \"Normalization\" (= class \"MinMaxScaler\"), rather than \"Standardisation\".\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "sc = MinMaxScaler(feature_range = (0, 1)) #feature_range = (0, 1), copy = True : these are the default values \n",
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08581368],\n",
       "       [0.09701243],\n",
       "       [0.09433366],\n",
       "       ...,\n",
       "       [0.95725128],\n",
       "       [0.93796041],\n",
       "       [0.93688146]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled #all the stock prices are now scaled between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JyYgYocqhNUg"
   },
   "source": [
    "### Creating a data structure with 60 timesteps and 1 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iofU21B0i6ST"
   },
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(60, len(training_set)):\n",
    "    x_train.append(training_set_scaled[i-60:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08581368, 0.09701243, 0.09433366, ..., 0.07846566, 0.08034452,\n",
       "        0.08497656],\n",
       "       [0.09701243, 0.09433366, 0.09156187, ..., 0.08034452, 0.08497656,\n",
       "        0.08627874],\n",
       "       [0.09433366, 0.09156187, 0.07984225, ..., 0.08497656, 0.08627874,\n",
       "        0.08471612],\n",
       "       ...,\n",
       "       [0.92106928, 0.92438053, 0.93048218, ..., 0.95475854, 0.95204256,\n",
       "        0.95163331],\n",
       "       [0.92438053, 0.93048218, 0.9299055 , ..., 0.95204256, 0.95163331,\n",
       "        0.95725128],\n",
       "       [0.93048218, 0.9299055 , 0.93113327, ..., 0.95163331, 0.95725128,\n",
       "        0.93796041]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08627874, 0.08471612, 0.07454052, ..., 0.95725128, 0.93796041,\n",
       "       0.93688146])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D8yaN7Zvi95l"
   },
   "source": [
    "### Reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1198, 60)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape #But we should reshape this to a 3D tensor as said in Keras API for input_shape for a RNN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FOXqJHmNjBkz"
   },
   "outputs": [],
   "source": [
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1)) #\"1\": the number of indicators (ex: APPLE, SAMSUNG, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.08581368],\n",
       "        [0.09701243],\n",
       "        [0.09433366],\n",
       "        ...,\n",
       "        [0.07846566],\n",
       "        [0.08034452],\n",
       "        [0.08497656]],\n",
       "\n",
       "       [[0.09701243],\n",
       "        [0.09433366],\n",
       "        [0.09156187],\n",
       "        ...,\n",
       "        [0.08034452],\n",
       "        [0.08497656],\n",
       "        [0.08627874]],\n",
       "\n",
       "       [[0.09433366],\n",
       "        [0.09156187],\n",
       "        [0.07984225],\n",
       "        ...,\n",
       "        [0.08497656],\n",
       "        [0.08627874],\n",
       "        [0.08471612]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.92106928],\n",
       "        [0.92438053],\n",
       "        [0.93048218],\n",
       "        ...,\n",
       "        [0.95475854],\n",
       "        [0.95204256],\n",
       "        [0.95163331]],\n",
       "\n",
       "       [[0.92438053],\n",
       "        [0.93048218],\n",
       "        [0.9299055 ],\n",
       "        ...,\n",
       "        [0.95204256],\n",
       "        [0.95163331],\n",
       "        [0.95725128]],\n",
       "\n",
       "       [[0.93048218],\n",
       "        [0.9299055 ],\n",
       "        [0.93113327],\n",
       "        ...,\n",
       "        [0.95163331],\n",
       "        [0.95725128],\n",
       "        [0.93796041]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZRRSOJeVjEWV"
   },
   "source": [
    "## Part 2 - Building and Training the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k4XV88JMjHXG"
   },
   "source": [
    "### Importing the Keras libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9JRnqsxEjKsD",
    "outputId": "b3e23e78-18aa-4396-c923-c877b8de60e4"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout #we can do these with Pytorch as well, especially for more professional tasks using dynamic graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FEIE-1s9jNzC"
   },
   "source": [
    "### Initialising the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1338dJ0UjRKH"
   },
   "outputs": [],
   "source": [
    "regressor = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "62eg1OPGjT8z"
   },
   "source": [
    "### Adding the first LSTM layer and some Dropout regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2hIinyXUjbVU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\nasa\\anaconda3\\envs\\mytfenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (x_train.shape[1], 1))) #return_sequences = True: IMP! to say that we are adding again the LSTM layers again.\n",
    "#usually in stock market applications, we choose \"units\" a high number, because it requires a high dimensiality to be able to predict well.\n",
    "#for \"input_shape\" we don't have to put the \"x_train.shape[0]\", because it is automatically taken into account. But we still have the 3D structure haa!\n",
    "\n",
    "regressor.add(Dropout(rate = 0.2)) #ignoring neurons by \"rate = 20%\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3XBIYLyOjlMx"
   },
   "source": [
    "### Adding a second LSTM layer and some Dropout regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UG7nrVaSjuZ2"
   },
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units = 50, return_sequences = True)) #now, since this is the second LSTM layer, we don't have to declare \"input_shape\" again. Because it is recognized automatically.\n",
    "\n",
    "regressor.add(Dropout(rate = 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ey3fHVnGj1cu"
   },
   "source": [
    "### Adding a third LSTM layer and some Dropout regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PuNi6PgFj7jO"
   },
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units = 50, return_sequences = True)) #now, since this is the second LSTM layer, we don't have to declare \"input_shape\" again. Because it is recognized automatically.\n",
    "\n",
    "regressor.add(Dropout(rate = 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SYTrtfTmj933"
   },
   "source": [
    "### Adding a fourth LSTM layer and some Dropout regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jp4Ty8fRkBYV"
   },
   "outputs": [],
   "source": [
    "regressor.add(LSTM(units = 50, return_sequences = False)) #now, since this is the second LSTM layer, we don't have to declare \"input_shape\" again. Because it is recognized automatically.\n",
    "#return_sequences = False: Because the next layer is the output layer!!\n",
    "\n",
    "regressor.add(Dropout(rate = 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ABI6rOIkHhk"
   },
   "source": [
    "### Adding the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aM6R1z4WkME8"
   },
   "outputs": [],
   "source": [
    "regressor.add(Dense(units = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zLx4K7uUkPSh"
   },
   "source": [
    "### Compiling the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XTrhVN-tkbko"
   },
   "outputs": [],
   "source": [
    "#It is recommended to use \"optimizer = RMSprop\" for RNN. But by experince, \"adam\" works fine here and is always a safe choice. Both of these work fine!\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error') #REMINDER: for Classification problems: \"loss = 'binary_crossentropy'\". But for Regression problems: \"loss = 'mean_squared_error'\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-mPhwKGkkebi"
   },
   "source": [
    "### Fitting the RNN to the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "I06Nkrz5kkb-",
    "outputId": "dc9d947a-4d5c-4dbc-ed45-31bf44b98dfb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\nasa\\anaconda3\\envs\\mytfenv\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 1198 samples\n",
      "Epoch 1/100\n",
      "1198/1198 [==============================] - 11s 9ms/sample - loss: 0.0513\n",
      "Epoch 2/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0063\n",
      "Epoch 3/100\n",
      "1198/1198 [==============================] - 8s 6ms/sample - loss: 0.0049\n",
      "Epoch 4/100\n",
      "1198/1198 [==============================] - 9s 7ms/sample - loss: 0.0042\n",
      "Epoch 5/100\n",
      "1198/1198 [==============================] - 9s 7ms/sample - loss: 0.0043\n",
      "Epoch 6/100\n",
      "1198/1198 [==============================] - 8s 6ms/sample - loss: 0.0043\n",
      "Epoch 7/100\n",
      "1198/1198 [==============================] - 10s 8ms/sample - loss: 0.0044\n",
      "Epoch 8/100\n",
      "1198/1198 [==============================] - 15s 13ms/sample - loss: 0.0046\n",
      "Epoch 9/100\n",
      "1198/1198 [==============================] - 14s 12ms/sample - loss: 0.0036\n",
      "Epoch 10/100\n",
      "1198/1198 [==============================] - 12s 10ms/sample - loss: 0.0042\n",
      "Epoch 11/100\n",
      "1198/1198 [==============================] - 14s 12ms/sample - loss: 0.0042\n",
      "Epoch 12/100\n",
      "1198/1198 [==============================] - 13s 10ms/sample - loss: 0.0044\n",
      "Epoch 13/100\n",
      "1198/1198 [==============================] - 13s 11ms/sample - loss: 0.0044\n",
      "Epoch 14/100\n",
      "1198/1198 [==============================] - 14s 11ms/sample - loss: 0.0036\n",
      "Epoch 15/100\n",
      "1198/1198 [==============================] - 17s 14ms/sample - loss: 0.0034\n",
      "Epoch 16/100\n",
      "1198/1198 [==============================] - 14s 12ms/sample - loss: 0.0037\n",
      "Epoch 17/100\n",
      "1198/1198 [==============================] - 14s 12ms/sample - loss: 0.0036\n",
      "Epoch 18/100\n",
      "1198/1198 [==============================] - 11s 9ms/sample - loss: 0.0035\n",
      "Epoch 19/100\n",
      "1198/1198 [==============================] - 10s 8ms/sample - loss: 0.0031\n",
      "Epoch 20/100\n",
      "1198/1198 [==============================] - 11s 9ms/sample - loss: 0.0034\n",
      "Epoch 21/100\n",
      "1198/1198 [==============================] - 13s 11ms/sample - loss: 0.0031\n",
      "Epoch 22/100\n",
      "1198/1198 [==============================] - 12s 10ms/sample - loss: 0.0028\n",
      "Epoch 23/100\n",
      "1198/1198 [==============================] - 13s 11ms/sample - loss: 0.0034\n",
      "Epoch 24/100\n",
      "1198/1198 [==============================] - 11s 9ms/sample - loss: 0.0030\n",
      "Epoch 25/100\n",
      "1198/1198 [==============================] - 12s 10ms/sample - loss: 0.0029\n",
      "Epoch 26/100\n",
      "1198/1198 [==============================] - 11s 9ms/sample - loss: 0.0031\n",
      "Epoch 27/100\n",
      "1198/1198 [==============================] - 11s 9ms/sample - loss: 0.0033\n",
      "Epoch 28/100\n",
      "1198/1198 [==============================] - 16s 13ms/sample - loss: 0.0029\n",
      "Epoch 29/100\n",
      "1198/1198 [==============================] - 13s 11ms/sample - loss: 0.0034\n",
      "Epoch 30/100\n",
      "1198/1198 [==============================] - 8s 6ms/sample - loss: 0.0027\n",
      "Epoch 31/100\n",
      "1198/1198 [==============================] - 8s 6ms/sample - loss: 0.0034\n",
      "Epoch 32/100\n",
      "1198/1198 [==============================] - 9s 7ms/sample - loss: 0.0029\n",
      "Epoch 33/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0029 0s - loss: 0.00\n",
      "Epoch 34/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0028\n",
      "Epoch 35/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0025\n",
      "Epoch 36/100\n",
      "1198/1198 [==============================] - 7s 5ms/sample - loss: 0.0026 0s - loss: 0.0\n",
      "Epoch 37/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0026\n",
      "Epoch 38/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0026\n",
      "Epoch 39/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0033\n",
      "Epoch 40/100\n",
      "1198/1198 [==============================] - 8s 7ms/sample - loss: 0.0029\n",
      "Epoch 41/100\n",
      "1198/1198 [==============================] - 8s 6ms/sample - loss: 0.0032\n",
      "Epoch 42/100\n",
      "1198/1198 [==============================] - 8s 7ms/sample - loss: 0.0026\n",
      "Epoch 43/100\n",
      "1198/1198 [==============================] - 8s 7ms/sample - loss: 0.0024\n",
      "Epoch 44/100\n",
      "1198/1198 [==============================] - 8s 7ms/sample - loss: 0.0028\n",
      "Epoch 45/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0022\n",
      "Epoch 46/100\n",
      "1198/1198 [==============================] - 8s 7ms/sample - loss: 0.0026\n",
      "Epoch 47/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0027 0s - loss: 0.\n",
      "Epoch 48/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0027\n",
      "Epoch 49/100\n",
      "1198/1198 [==============================] - 8s 7ms/sample - loss: 0.0025\n",
      "Epoch 50/100\n",
      "1198/1198 [==============================] - 8s 7ms/sample - loss: 0.0027 0s - loss: 0.0\n",
      "Epoch 51/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0022\n",
      "Epoch 52/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0022\n",
      "Epoch 53/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0023\n",
      "Epoch 54/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0023\n",
      "Epoch 55/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0021\n",
      "Epoch 56/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0022\n",
      "Epoch 57/100\n",
      "1198/1198 [==============================] - 9s 7ms/sample - loss: 0.0021 4s - loss: 0.002 - ETA: 4\n",
      "Epoch 58/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0023\n",
      "Epoch 59/100\n",
      "1198/1198 [==============================] - 8s 6ms/sample - loss: 0.0023\n",
      "Epoch 60/100\n",
      "1198/1198 [==============================] - 8s 7ms/sample - loss: 0.0024\n",
      "Epoch 61/100\n",
      "1198/1198 [==============================] - 8s 7ms/sample - loss: 0.0021\n",
      "Epoch 62/100\n",
      "1198/1198 [==============================] - 8s 7ms/sample - loss: 0.0021\n",
      "Epoch 63/100\n",
      "1198/1198 [==============================] - 10s 8ms/sample - loss: 0.0023\n",
      "Epoch 64/100\n",
      "1198/1198 [==============================] - 8s 6ms/sample - loss: 0.0020\n",
      "Epoch 65/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0022\n",
      "Epoch 66/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0023\n",
      "Epoch 67/100\n",
      "1198/1198 [==============================] - 8s 7ms/sample - loss: 0.0018\n",
      "Epoch 68/100\n",
      "1198/1198 [==============================] - 8s 7ms/sample - loss: 0.0023\n",
      "Epoch 69/100\n",
      "1198/1198 [==============================] - 9s 8ms/sample - loss: 0.0017\n",
      "Epoch 70/100\n",
      "1198/1198 [==============================] - 8s 7ms/sample - loss: 0.0020\n",
      "Epoch 71/100\n",
      "1198/1198 [==============================] - 9s 7ms/sample - loss: 0.0019\n",
      "Epoch 72/100\n",
      "1198/1198 [==============================] - 8s 7ms/sample - loss: 0.0018\n",
      "Epoch 73/100\n",
      "1198/1198 [==============================] - 10s 8ms/sample - loss: 0.0020\n",
      "Epoch 74/100\n",
      "1198/1198 [==============================] - 9s 7ms/sample - loss: 0.0018\n",
      "Epoch 75/100\n",
      "1198/1198 [==============================] - 9s 7ms/sample - loss: 0.0018\n",
      "Epoch 76/100\n",
      "1198/1198 [==============================] - 9s 7ms/sample - loss: 0.0018\n",
      "Epoch 77/100\n",
      "1198/1198 [==============================] - 8s 7ms/sample - loss: 0.0018\n",
      "Epoch 78/100\n",
      "1198/1198 [==============================] - 9s 7ms/sample - loss: 0.0017\n",
      "Epoch 79/100\n",
      "1198/1198 [==============================] - 9s 7ms/sample - loss: 0.0016\n",
      "Epoch 80/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0016\n",
      "Epoch 81/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0017\n",
      "Epoch 82/100\n",
      "1198/1198 [==============================] - 9s 8ms/sample - loss: 0.0017\n",
      "Epoch 83/100\n",
      "1198/1198 [==============================] - 9s 7ms/sample - loss: 0.0016\n",
      "Epoch 84/100\n",
      "1198/1198 [==============================] - 10s 8ms/sample - loss: 0.0019\n",
      "Epoch 85/100\n",
      "1198/1198 [==============================] - 10s 9ms/sample - loss: 0.0019\n",
      "Epoch 86/100\n",
      "1198/1198 [==============================] - 8s 6ms/sample - loss: 0.0017\n",
      "Epoch 87/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0014 \n",
      "Epoch 88/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0016\n",
      "Epoch 89/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0018\n",
      "Epoch 90/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0015\n",
      "Epoch 91/100\n",
      "1198/1198 [==============================] - 7s 5ms/sample - loss: 0.0017 2s -\n",
      "Epoch 92/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0016\n",
      "Epoch 93/100\n",
      "1198/1198 [==============================] - 8s 6ms/sample - loss: 0.0017 2s - l\n",
      "Epoch 94/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0016\n",
      "Epoch 95/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0013\n",
      "Epoch 96/100\n",
      "1198/1198 [==============================] - 8s 7ms/sample - loss: 0.0016\n",
      "Epoch 97/100\n",
      "1198/1198 [==============================] - 7s 6ms/sample - loss: 0.0014\n",
      "Epoch 98/100\n",
      "1198/1198 [==============================] - 8s 7ms/sample - loss: 0.0016\n",
      "Epoch 99/100\n",
      "1198/1198 [==============================] - 8s 6ms/sample - loss: 0.0015\n",
      "Epoch 100/100\n",
      "1198/1198 [==============================] - 8s 7ms/sample - loss: 0.0015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15095443f48>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.fit(x_train, y_train, epochs = 100, batch_size = 32) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4hRau_lIkrE8"
   },
   "source": [
    "## Part 3 - Making the predictions and visualising the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SgJO6qEDksxD"
   },
   "source": [
    "### Getting the real stock price of 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FmBT2zqukxTz"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('Google_Stock_Price_Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/3/2017</td>\n",
       "      <td>778.81</td>\n",
       "      <td>789.63</td>\n",
       "      <td>775.80</td>\n",
       "      <td>786.14</td>\n",
       "      <td>1,657,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1/4/2017</td>\n",
       "      <td>788.36</td>\n",
       "      <td>791.34</td>\n",
       "      <td>783.16</td>\n",
       "      <td>786.90</td>\n",
       "      <td>1,073,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1/5/2017</td>\n",
       "      <td>786.08</td>\n",
       "      <td>794.48</td>\n",
       "      <td>785.02</td>\n",
       "      <td>794.02</td>\n",
       "      <td>1,335,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1/6/2017</td>\n",
       "      <td>795.26</td>\n",
       "      <td>807.90</td>\n",
       "      <td>792.20</td>\n",
       "      <td>806.15</td>\n",
       "      <td>1,640,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1/9/2017</td>\n",
       "      <td>806.40</td>\n",
       "      <td>809.97</td>\n",
       "      <td>802.83</td>\n",
       "      <td>806.65</td>\n",
       "      <td>1,272,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1/10/2017</td>\n",
       "      <td>807.86</td>\n",
       "      <td>809.13</td>\n",
       "      <td>803.51</td>\n",
       "      <td>804.79</td>\n",
       "      <td>1,176,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1/11/2017</td>\n",
       "      <td>805.00</td>\n",
       "      <td>808.15</td>\n",
       "      <td>801.37</td>\n",
       "      <td>807.91</td>\n",
       "      <td>1,065,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1/12/2017</td>\n",
       "      <td>807.14</td>\n",
       "      <td>807.39</td>\n",
       "      <td>799.17</td>\n",
       "      <td>806.36</td>\n",
       "      <td>1,353,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1/13/2017</td>\n",
       "      <td>807.48</td>\n",
       "      <td>811.22</td>\n",
       "      <td>806.69</td>\n",
       "      <td>807.88</td>\n",
       "      <td>1,099,200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1/17/2017</td>\n",
       "      <td>807.08</td>\n",
       "      <td>807.14</td>\n",
       "      <td>800.37</td>\n",
       "      <td>804.61</td>\n",
       "      <td>1,362,100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1/18/2017</td>\n",
       "      <td>805.81</td>\n",
       "      <td>806.21</td>\n",
       "      <td>800.99</td>\n",
       "      <td>806.07</td>\n",
       "      <td>1,294,400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1/19/2017</td>\n",
       "      <td>805.12</td>\n",
       "      <td>809.48</td>\n",
       "      <td>801.80</td>\n",
       "      <td>802.17</td>\n",
       "      <td>919,300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1/20/2017</td>\n",
       "      <td>806.91</td>\n",
       "      <td>806.91</td>\n",
       "      <td>801.69</td>\n",
       "      <td>805.02</td>\n",
       "      <td>1,670,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1/23/2017</td>\n",
       "      <td>807.25</td>\n",
       "      <td>820.87</td>\n",
       "      <td>803.74</td>\n",
       "      <td>819.31</td>\n",
       "      <td>1,963,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1/24/2017</td>\n",
       "      <td>822.30</td>\n",
       "      <td>825.90</td>\n",
       "      <td>817.82</td>\n",
       "      <td>823.87</td>\n",
       "      <td>1,474,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1/25/2017</td>\n",
       "      <td>829.62</td>\n",
       "      <td>835.77</td>\n",
       "      <td>825.06</td>\n",
       "      <td>835.67</td>\n",
       "      <td>1,494,500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1/26/2017</td>\n",
       "      <td>837.81</td>\n",
       "      <td>838.00</td>\n",
       "      <td>827.01</td>\n",
       "      <td>832.15</td>\n",
       "      <td>2,973,900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1/27/2017</td>\n",
       "      <td>834.71</td>\n",
       "      <td>841.95</td>\n",
       "      <td>820.44</td>\n",
       "      <td>823.31</td>\n",
       "      <td>2,965,800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1/30/2017</td>\n",
       "      <td>814.66</td>\n",
       "      <td>815.84</td>\n",
       "      <td>799.80</td>\n",
       "      <td>802.32</td>\n",
       "      <td>3,246,600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1/31/2017</td>\n",
       "      <td>796.86</td>\n",
       "      <td>801.25</td>\n",
       "      <td>790.52</td>\n",
       "      <td>796.79</td>\n",
       "      <td>2,160,600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date    Open    High     Low   Close     Volume\n",
       "0    1/3/2017  778.81  789.63  775.80  786.14  1,657,300\n",
       "1    1/4/2017  788.36  791.34  783.16  786.90  1,073,000\n",
       "2    1/5/2017  786.08  794.48  785.02  794.02  1,335,200\n",
       "3    1/6/2017  795.26  807.90  792.20  806.15  1,640,200\n",
       "4    1/9/2017  806.40  809.97  802.83  806.65  1,272,400\n",
       "5   1/10/2017  807.86  809.13  803.51  804.79  1,176,800\n",
       "6   1/11/2017  805.00  808.15  801.37  807.91  1,065,900\n",
       "7   1/12/2017  807.14  807.39  799.17  806.36  1,353,100\n",
       "8   1/13/2017  807.48  811.22  806.69  807.88  1,099,200\n",
       "9   1/17/2017  807.08  807.14  800.37  804.61  1,362,100\n",
       "10  1/18/2017  805.81  806.21  800.99  806.07  1,294,400\n",
       "11  1/19/2017  805.12  809.48  801.80  802.17    919,300\n",
       "12  1/20/2017  806.91  806.91  801.69  805.02  1,670,000\n",
       "13  1/23/2017  807.25  820.87  803.74  819.31  1,963,600\n",
       "14  1/24/2017  822.30  825.90  817.82  823.87  1,474,000\n",
       "15  1/25/2017  829.62  835.77  825.06  835.67  1,494,500\n",
       "16  1/26/2017  837.81  838.00  827.01  832.15  2,973,900\n",
       "17  1/27/2017  834.71  841.95  820.44  823.31  2,965,800\n",
       "18  1/30/2017  814.66  815.84  799.80  802.32  3,246,600\n",
       "19  1/31/2017  796.86  801.25  790.52  796.79  2,160,600"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = df_test.iloc[:, 1:2].values #real stock price (for January 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[778.81],\n",
       "       [788.36],\n",
       "       [786.08],\n",
       "       [795.26],\n",
       "       [806.4 ],\n",
       "       [807.86],\n",
       "       [805.  ],\n",
       "       [807.14],\n",
       "       [807.48],\n",
       "       [807.08],\n",
       "       [805.81],\n",
       "       [805.12],\n",
       "       [806.91],\n",
       "       [807.25],\n",
       "       [822.3 ],\n",
       "       [829.62],\n",
       "       [837.81],\n",
       "       [834.71],\n",
       "       [814.66],\n",
       "       [796.86]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set #This is the real google stock price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GrvrLblxkz42"
   },
   "source": [
    "### Getting the predicted stock price of 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emikTvUpk3Ck"
   },
   "outputs": [],
   "source": [
    "#the new datset, containing \"training_set\" and then \"test_set\" at its end:\n",
    "\n",
    "df_total = pd.concat((df_train['Open'], df_test['Open']), axis = 0) #adding (concatenating) the training set and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1278,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total\n",
    "df_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"inputs\", containing 80 rows that we want (60 from \"training_set\" + 20 from \"test_set\"):\n",
    "\n",
    "inputs = df_total[len(df_total) - len(df_test) - 60 : ].values #choosing the last 60 days input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([779.  , 779.66, 777.71, 786.66, 783.76, 781.22, 781.65, 779.8 ,\n",
       "       787.85, 798.24, 803.3 , 795.  , 804.9 , 816.68, 806.34, 801.  ,\n",
       "       808.35, 795.47, 782.89, 778.2 , 767.25, 750.66, 774.5 , 783.4 ,\n",
       "       779.94, 791.17, 756.54, 755.6 , 746.97, 755.2 , 766.92, 771.37,\n",
       "       762.61, 772.63, 767.73, 764.26, 760.  , 771.53, 770.07, 757.44,\n",
       "       744.59, 757.71, 764.73, 761.  , 772.48, 780.  , 785.04, 793.9 ,\n",
       "       797.4 , 797.34, 800.4 , 790.22, 796.76, 795.84, 792.36, 790.9 ,\n",
       "       790.68, 793.7 , 783.33, 782.75, 778.81, 788.36, 786.08, 795.26,\n",
       "       806.4 , 807.86, 805.  , 807.14, 807.48, 807.08, 805.81, 805.12,\n",
       "       806.91, 807.25, 822.3 , 829.62, 837.81, 834.71, 814.66, 796.86])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape \"inputs\" from horizontal to vertical!!\n",
    "\n",
    "inputs = inputs.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[779.  ],\n",
       "       [779.66],\n",
       "       [777.71],\n",
       "       [786.66],\n",
       "       [783.76],\n",
       "       [781.22],\n",
       "       [781.65],\n",
       "       [779.8 ],\n",
       "       [787.85],\n",
       "       [798.24],\n",
       "       [803.3 ],\n",
       "       [795.  ],\n",
       "       [804.9 ],\n",
       "       [816.68],\n",
       "       [806.34],\n",
       "       [801.  ],\n",
       "       [808.35],\n",
       "       [795.47],\n",
       "       [782.89],\n",
       "       [778.2 ],\n",
       "       [767.25],\n",
       "       [750.66],\n",
       "       [774.5 ],\n",
       "       [783.4 ],\n",
       "       [779.94],\n",
       "       [791.17],\n",
       "       [756.54],\n",
       "       [755.6 ],\n",
       "       [746.97],\n",
       "       [755.2 ],\n",
       "       [766.92],\n",
       "       [771.37],\n",
       "       [762.61],\n",
       "       [772.63],\n",
       "       [767.73],\n",
       "       [764.26],\n",
       "       [760.  ],\n",
       "       [771.53],\n",
       "       [770.07],\n",
       "       [757.44],\n",
       "       [744.59],\n",
       "       [757.71],\n",
       "       [764.73],\n",
       "       [761.  ],\n",
       "       [772.48],\n",
       "       [780.  ],\n",
       "       [785.04],\n",
       "       [793.9 ],\n",
       "       [797.4 ],\n",
       "       [797.34],\n",
       "       [800.4 ],\n",
       "       [790.22],\n",
       "       [796.76],\n",
       "       [795.84],\n",
       "       [792.36],\n",
       "       [790.9 ],\n",
       "       [790.68],\n",
       "       [793.7 ],\n",
       "       [783.33],\n",
       "       [782.75],\n",
       "       [778.81],\n",
       "       [788.36],\n",
       "       [786.08],\n",
       "       [795.26],\n",
       "       [806.4 ],\n",
       "       [807.86],\n",
       "       [805.  ],\n",
       "       [807.14],\n",
       "       [807.48],\n",
       "       [807.08],\n",
       "       [805.81],\n",
       "       [805.12],\n",
       "       [806.91],\n",
       "       [807.25],\n",
       "       [822.3 ],\n",
       "       [829.62],\n",
       "       [837.81],\n",
       "       [834.71],\n",
       "       [814.66],\n",
       "       [796.86]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling the inputs:\n",
    "\n",
    "inputs = sc.transform(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9299055 ],\n",
       "       [0.93113327],\n",
       "       [0.92750577],\n",
       "       [0.94415507],\n",
       "       [0.93876032],\n",
       "       [0.93403527],\n",
       "       [0.93483518],\n",
       "       [0.9313937 ],\n",
       "       [0.94636878],\n",
       "       [0.96569685],\n",
       "       [0.97510976],\n",
       "       [0.95966962],\n",
       "       [0.97808617],\n",
       "       [1.        ],\n",
       "       [0.98076494],\n",
       "       [0.97083116],\n",
       "       [0.98450406],\n",
       "       [0.96054394],\n",
       "       [0.9371419 ],\n",
       "       [0.92841729],\n",
       "       [0.90804747],\n",
       "       [0.8771858 ],\n",
       "       [0.92153434],\n",
       "       [0.93809063],\n",
       "       [0.93165414],\n",
       "       [0.95254483],\n",
       "       [0.88812412],\n",
       "       [0.88637547],\n",
       "       [0.87032145],\n",
       "       [0.88563137],\n",
       "       [0.90743359],\n",
       "       [0.91571173],\n",
       "       [0.89941588],\n",
       "       [0.91805566],\n",
       "       [0.9089404 ],\n",
       "       [0.9024853 ],\n",
       "       [0.89456061],\n",
       "       [0.91600938],\n",
       "       [0.9132934 ],\n",
       "       [0.88979835],\n",
       "       [0.86589404],\n",
       "       [0.89030062],\n",
       "       [0.90335962],\n",
       "       [0.89642086],\n",
       "       [0.91777662],\n",
       "       [0.93176576],\n",
       "       [0.94114145],\n",
       "       [0.95762334],\n",
       "       [0.96413424],\n",
       "       [0.96402262],\n",
       "       [0.96971501],\n",
       "       [0.95077759],\n",
       "       [0.96294367],\n",
       "       [0.96123223],\n",
       "       [0.95475854],\n",
       "       [0.95204256],\n",
       "       [0.95163331],\n",
       "       [0.95725128],\n",
       "       [0.93796041],\n",
       "       [0.93688146],\n",
       "       [0.92955205],\n",
       "       [0.94731751],\n",
       "       [0.94307612],\n",
       "       [0.96015329],\n",
       "       [0.98087655],\n",
       "       [0.98359253],\n",
       "       [0.97827219],\n",
       "       [0.98225314],\n",
       "       [0.98288563],\n",
       "       [0.98214153],\n",
       "       [0.979779  ],\n",
       "       [0.97849542],\n",
       "       [0.98182528],\n",
       "       [0.98245777],\n",
       "       [1.01045465],\n",
       "       [1.02407173],\n",
       "       [1.03930724],\n",
       "       [1.03354044],\n",
       "       [0.99624228],\n",
       "       [0.9631297 ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperating \"x_train\" and \"y_train\":\n",
    "\n",
    "x_test = []\n",
    "# y_test = [] #we delete \"y_test\", because we do not need to know the exact values of ut since we are only predicting it.\n",
    "for i in range(60, len(inputs)):\n",
    "    x_test.append(inputs[i-60:i, 0])\n",
    "#     y_test.append(inputs[i, 0])\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "# y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9299055 , 0.93113327, 0.92750577, ..., 0.95725128, 0.93796041,\n",
       "        0.93688146],\n",
       "       [0.93113327, 0.92750577, 0.94415507, ..., 0.93796041, 0.93688146,\n",
       "        0.92955205],\n",
       "       [0.92750577, 0.94415507, 0.93876032, ..., 0.93688146, 0.92955205,\n",
       "        0.94731751],\n",
       "       ...,\n",
       "       [0.96054394, 0.9371419 , 0.92841729, ..., 1.01045465, 1.02407173,\n",
       "        1.03930724],\n",
       "       [0.9371419 , 0.92841729, 0.90804747, ..., 1.02407173, 1.03930724,\n",
       "        1.03354044],\n",
       "       [0.92841729, 0.90804747, 0.8771858 , ..., 1.03930724, 1.03354044,\n",
       "        0.99624228]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 60)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to 3D, just like before:\n",
    "\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1)) #\"1\": the number of indicators (ex: APPLE, SAMSUNG, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.9299055 ],\n",
       "        [0.93113327],\n",
       "        [0.92750577],\n",
       "        ...,\n",
       "        [0.95725128],\n",
       "        [0.93796041],\n",
       "        [0.93688146]],\n",
       "\n",
       "       [[0.93113327],\n",
       "        [0.92750577],\n",
       "        [0.94415507],\n",
       "        ...,\n",
       "        [0.93796041],\n",
       "        [0.93688146],\n",
       "        [0.92955205]],\n",
       "\n",
       "       [[0.92750577],\n",
       "        [0.94415507],\n",
       "        [0.93876032],\n",
       "        ...,\n",
       "        [0.93688146],\n",
       "        [0.92955205],\n",
       "        [0.94731751]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.96054394],\n",
       "        [0.9371419 ],\n",
       "        [0.92841729],\n",
       "        ...,\n",
       "        [1.01045465],\n",
       "        [1.02407173],\n",
       "        [1.03930724]],\n",
       "\n",
       "       [[0.9371419 ],\n",
       "        [0.92841729],\n",
       "        [0.90804747],\n",
       "        ...,\n",
       "        [1.02407173],\n",
       "        [1.03930724],\n",
       "        [1.03354044]],\n",
       "\n",
       "       [[0.92841729],\n",
       "        [0.90804747],\n",
       "        [0.8771858 ],\n",
       "        ...,\n",
       "        [1.03930724],\n",
       "        [1.03354044],\n",
       "        [0.99624228]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, the prediction:\n",
    "\n",
    "y_pred = regressor.predict(x_test) #predicted google stock price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91423976],\n",
       "       [0.90889347],\n",
       "       [0.907797  ],\n",
       "       [0.90998566],\n",
       "       [0.916257  ],\n",
       "       [0.9273548 ],\n",
       "       [0.9387847 ],\n",
       "       [0.94499373],\n",
       "       [0.94619083],\n",
       "       [0.94472826],\n",
       "       [0.9427762 ],\n",
       "       [0.94126797],\n",
       "       [0.94052064],\n",
       "       [0.9412801 ],\n",
       "       [0.9431342 ],\n",
       "       [0.9506208 ],\n",
       "       [0.96316314],\n",
       "       [0.97784686],\n",
       "       [0.98764074],\n",
       "       [0.9830196 ]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = sc.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[770.5787 ],\n",
       "       [767.7048 ],\n",
       "       [767.1153 ],\n",
       "       [768.2919 ],\n",
       "       [771.6631 ],\n",
       "       [777.62885],\n",
       "       [783.7731 ],\n",
       "       [787.1108 ],\n",
       "       [787.75433],\n",
       "       [786.9681 ],\n",
       "       [785.91876],\n",
       "       [785.108  ],\n",
       "       [784.70624],\n",
       "       [785.1145 ],\n",
       "       [786.1112 ],\n",
       "       [790.1357 ],\n",
       "       [796.8779 ],\n",
       "       [804.77136],\n",
       "       [810.03613],\n",
       "       [807.552  ]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred #These are the predicted google stock price for January 2017!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iFTNs3YHk6FQ"
   },
   "source": [
    "### Visualising the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "8OUI8U49k9tH",
    "outputId": "db0ed15e-071b-4bae-955e-0eda3df95238"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1509e8e68c8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_set, color = 'red', label = 'real google stock price')\n",
    "plt.plot(y_pred, color = 'blue', label = 'predicted google stock price')\n",
    "\n",
    "plt.title('google stock price prediction')\n",
    "plt.xlabel('time (month 1 January to 30 January)')\n",
    "plt.ylabel('price')\n",
    "plt.legend() #we HAVE TO add thid line, so that the \"legend\" in the plot is added to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As seen above, the model lags behind becaue it can not react fast enough to fast, non-linear changes. \n",
    "#But it reacts well to smooth changes, and manages to follow the upward and downward trends."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "recurrent_neural_network.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
